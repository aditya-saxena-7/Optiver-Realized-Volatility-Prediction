# -*- coding: utf-8 -*-
"""Features Construction and EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13FDwsepcZXs611qnkf5EUoRrF3HIal4V
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')

"""# Order book data EDA

1. **File Path Definition**:
   ```python
   path = r'./book_train.parquet/stock_id=0'
   ```
   This line sets the file path for the dataset, which in this case is a parquet file containing order book data for `stock_id=0`.

2. **Log Return Function**:
   ```python
   def log_return(list_stock_prices):
       return np.log(list_stock_prices).diff()
   ```
   This function computes the logarithmic returns of a series of stock prices. The logarithmic return is a common transformation in financial data analysis because it normalizes relative changes in price, which makes some statistical analyses more straightforward.

3. **Realized Volatility Calculation**:
   ```python
   def realized_volatility(series_log_return):
       return np.sqrt(np.sum(series_log_return**2))
   ```
   This function calculates the realized volatility of a series of log returns by summing the squares of the returns and then taking the square root, which measures the standard deviation of the stock's returns—a common volatility metric.

4. **Feature Construction from Order Book Data**:
   ```python
   def order_book_data_feature_construction(path):
       stock_id = path.split("=")[1]
       example_book_df = pd.read_parquet(path)
   ```
   This function starts by extracting the stock ID from the file path and reads the parquet file into a DataFrame.

   **Detailed Feature Engineering**:
   - **Spreads and Volumes**:
     Constructs features like `ask_spread`, `bid_spread`, `total_volume`, and `volume_imbalance` from the sizes of the best (level 1) and second-best (level 2) bid and ask orders. These features capture aspects of market depth and order book pressure.
   
   - **Weighted Average Price (WAP)**:
     Calculates two levels of weighted average prices (WAP1 and WAP2), which are used to compute the price at which most trades could happen given the current order book. It also calculates `wap_balance` to capture the difference between these two prices, which can indicate shifts in market sentiment or liquidity.
   
   - **Log Returns**:
     Applies the `log_return` function to the WAP columns grouped by `time_id` to get the log returns at each level, which are critical for volatility calculations.
     ```python
     example_book_df.loc[:,('log_return1')] = example_book_df.groupby('time_id')['wap1'].apply(log_return)
     example_book_df.loc[:,('log_return2')] = example_book_df.groupby('time_id')['wap2'].apply(log_return)
     example_book_df = example_book_df[~example_book_df['log_return1'].isnull()]  # drop the first null return value
     ```

5. **Return Processed DataFrame**:
   Finally, the processed DataFrame with all the newly created features is returned.

Certainly! Let’s delve into why logarithmic returns and realized volatility are crucial concepts in financial analysis, especially in the context of modeling and predicting stock market behavior.

### Importance of Logarithmic Returns

1. **Normalization of Relative Changes**: Logarithmic returns help normalize percentage changes in prices, which can vary widely in magnitude across different price levels. For instance, a $1 change in a $10 stock is much more significant than a $1 change in a $100 stock. Log returns convert these changes into comparable metrics.

2. **Time Additivity**: A key mathematical convenience of log returns is their additivity over time. This means that the log return over a period is simply the sum of the log returns of the sub-periods. This property is particularly useful in financial modeling where aggregation and comparison of returns over different time intervals are common.

3. **Handling Negative Prices**: Log returns are useful because they do not undefined or infinite values with negative price changes, as might happen with arithmetic returns if stock prices fall drastically.

4. **Statistical Properties**: Logarithmic transformations tend to stabilize the variance of financial time series and make them more conducive to the assumptions underlying many statistical modeling techniques, thus improving model accuracy and inference.

### Example of Logarithmic Returns Use:
Consider an investor tracking the price of a stock over three days, with prices at $100, $110, and $121 on consecutive days. The daily log returns would be calculated as follows:

- Day 1 to Day 2: \( \log(\frac{110}{100}) = \log(1.1) \)
- Day 2 to Day 3: \( \log(\frac{121}{110}) = \log(1.1) \)

The log return from Day 1 to Day 3 would be the sum of the individual daily log returns, reflecting a total change in a manner that is independent of the path the price takes.

### Importance of Realized Volatility

1. **Measure of Risk**: Realized volatility is a direct measure of the risk associated with an asset over a specific time frame. It provides a historical quantification of how much the price of an asset has varied, which is essential for risk management and strategic decision-making.

2. **Pricing of Derivatives**: In financial markets, the pricing of derivatives such as options heavily depends on volatility estimates. Higher volatility increases the likelihood of significant price swings, which can affect the premiums of options and other derivatives.

3. **Market Sentiment Indicator**: Changes in realized volatility can indicate shifts in market sentiment. For instance, a sudden increase in volatility might signal uncertainty or fear in the market, which can precede larger economic or sector-specific shifts.

4. **Strategic Trading**: Traders use realized volatility to adjust their trading strategies. For instance, a trader might choose more conservative strategies in times of high volatility to mitigate risk, or capitalize on the high volatility using options strategies designed to benefit from big price swings.

### Example of Realized Volatility Use:
Suppose an analyst is assessing the volatility of a stock using daily returns over a 10-day period. The analyst calculates the log returns for each day, squares each of these returns to focus on the magnitude (ignoring direction), and then sums these squared returns. The square root of this sum gives the realized volatility, which quantifies how much the stock’s returns deviated from the mean during this period. This measure helps in assessing the risk profile of the stock and could influence decisions on asset allocation in a portfolio.

Together, logarithmic returns and realized volatility form the backbone of quantitative finance by providing tools to model market dynamics more accurately, manage financial risk, and devise effective trading strategies.

The DataFrame `example_book_df`:

1. **time_id**: This identifier groups all the records that belong to a single trading day or specific time interval. It's essential for analyzing data related to specific events or time periods within the trading day.

2. **seconds_in_bucket**: Represents the number of seconds since the start of the trading session at which the data was sampled. This granular time stamp helps in studying price movements and order book changes at a very high resolution.

3. **bid_price1** and **bid_price2**: These columns show the highest (bid_price1) and second-highest (bid_price2) prices that buyers are willing to pay for the stock. These are critical in understanding the demand side of the market dynamics.

4. **ask_price1** and **ask_price2**: These are the lowest (ask_price1) and second-lowest (ask_price2) prices at which sellers are willing to sell the stock. These prices reflect the supply conditions of the market.

5. **bid_size1** and **bid_size2**: Indicate the number of shares buyers are willing to purchase at the corresponding bid prices (bid_price1 and bid_price2). These sizes can be used to gauge market depth and buying interest at different price levels.

6. **ask_size1** and **ask_size2**: Reflect the number of shares sellers are ready to sell at the corresponding ask prices (ask_price1 and ask_price2). Similar to bid sizes, these provide insights into the market depth on the selling side.

7. **ask_spread**: Calculated as `ask_size1 - ask_size2`, this represents the difference in volume between the closest and next-closest ask levels. It can indicate the liquidity or tightness of the ask side of the order book.

8. **bid_spread**: Calculated as `bid_size1 - bid_size2`, this indicates the volume difference between the top two bid levels. It helps assess the liquidity or tightness on the bid side of the order book.

9. **total_volume**: The sum of bid and ask sizes at both the first and second levels (`bid_size1 + ask_size1 + bid_size2 + ask_size2`). This metric provides a comprehensive view of the overall market depth.

10. **volume_imbalance**: The absolute difference between total bid volumes and total ask volumes (`abs(ask_size1 + ask_size2 - bid_size1 - bid_size2)`). It's a measure of the current supply-demand balance in the order book.

11. **price_spread**: Defined as `(ask_price1 / bid_price1) - 1`, this percentage shows the relative difference between the best ask and the best bid prices, reflecting the immediate market spread which impacts trading costs and market entry/exit.

12. **wap1** and **wap2**: Weighted average prices for the first and second levels of the order book, respectively. These are calculated by considering both price and available volume at each level, providing a more nuanced view of the current trading price than the best bid or ask alone.

13. **wap_balance**: The difference between the weighted average prices at the first and second levels (`wap1 - wap2`). This metric can indicate shifts in price levels and potential momentum in price movements.

14. **log_return1** and **log_return2**: The logarithmic returns calculated from the sequential weighted average prices at the first and second levels, respectively. These returns are crucial for analyzing the volatility and price movement trends over very short intervals.

The Weighted Average Price (WAP) is a crucial measure in financial markets, especially when dealing with high-frequency trading data. It provides a more refined and accurate estimation of the average trading price of a security at a given time, factoring in both price levels and the available volumes at these prices. This method of averaging is especially significant in order books, where multiple buy (bid) and sell (ask) orders exist at various price levels.

### Calculation of WAP
WAP is calculated using the following general formula:
\[ \text{WAP} = \frac{\sum (\text{Price} \times \text{Volume})}{\sum \text{Volume}} \]
This formula computes the average price based on trading volumes, where each price is weighted by the amount of the asset that is traded at that price. In an order book context, this might be broken down into different levels, typically capturing the top few layers of the book where most of the trading activity happens.

### Significance in the Example

In the dataset you provided, WAP is calculated for the first two levels of the order book:
- **wap1**: This is the weighted average price calculated using the first level of the bid and ask prices and volumes. It reflects the most competitive and likely trading prices, as these represent the highest bid and the lowest ask available.
- **wap2**: Similarly, this weighted average price is calculated for the second level of the bid and ask prices and volumes, which are the next best prices available after the top level.

The specific formulas based on your data description would be:
\[ \text{wap1} = \frac{\text{bid\_price1} \times \text{ask\_size1} + \text{ask\_price1} \times \text{bid\_size1}}{\text{ask\_size1} + \text{bid\_size1}} \]
\[ \text{wap2} = \frac{\text{bid\_price2} \times \text{ask\_size2} + \text{ask\_price2} \times \text{bid\_size2}}{\text{ask\_size2} + \text{bid\_size2}} \]

### Usage and Importance
- **Market Insight**: WAP provides a more accurate reflection of market price than simply looking at the last traded price or the best available bid and ask because it incorporates the volume available at these prices, offering a depth of insight into where the market values the stock.
- **Volatility and Trend Analysis**: Changes in WAP over time can be used to analyze trends and volatility in stock prices. If WAP moves significantly, it can indicate a shift in market sentiment or reaction to news events.
- **Trade Execution**: Traders might use WAP to gauge the effectiveness of their trade executions. Trading at prices better than the WAP might indicate effective execution, whereas consistently trading at worse prices might indicate slippage or ineffective trading strategies.
- **Benchmarking**: WAP can serve as a benchmark for measuring the performance of trading algorithms, particularly in strategies involving frequent buying and selling of financial instruments.

Understanding and analyzing WAP in the context of order book data is essential for anyone engaged in high-frequency trading, market making, or any trading strategy where understanding price dynamics and market depth is crucial.
"""

path = r'./book_train.parquet/stock_id=0'


def log_return(list_stock_prices):
    return np.log(list_stock_prices).diff()

def realized_volatility(series_log_return):
    return np.sqrt(np.sum(series_log_return**2))

def order_book_data_feature_construction(path):

    stock_id = path.split("=")[1]
    example_book_df = pd.read_parquet(path)

    # construct item for realized volatility calculation
    example_book_df.loc[:,('ask_spread')] = example_book_df['ask_size1'] - example_book_df['ask_size2']
    example_book_df.loc[:,('bid_spread')] = example_book_df['bid_size1'] - example_book_df['bid_size2']
    example_book_df.loc[:,('total_volume')] = example_book_df['ask_size1'] + example_book_df['ask_size2'] + example_book_df['bid_size1'] + example_book_df['bid_size2']
    example_book_df.loc[:,('volume_imbalance')] = abs(example_book_df['ask_size1'] + example_book_df['ask_size2'] - example_book_df['bid_size1'] - example_book_df['bid_size2'] )


    example_book_df.loc[:,('price_spread')] = (example_book_df['ask_price1']/example_book_df['bid_price1'])-1 # Constrcut Bid/Ask Spread.
    example_book_df.loc[:,('wap1')] = (example_book_df['bid_price1']*example_book_df['ask_size1'] + example_book_df['ask_price1']*example_book_df['bid_size1']) / (example_book_df['ask_size1'] + example_book_df['bid_size1'])
    example_book_df.loc[:,('wap2')] = (example_book_df['bid_price2']*example_book_df['ask_size2'] + example_book_df['ask_price2']*example_book_df['bid_size2']) / (example_book_df['ask_size2'] + example_book_df['bid_size2'])
    example_book_df.loc[:,('wap_balance')] = example_book_df['wap1'] - example_book_df['wap2']

    example_book_df.loc[:,('log_return1')] = example_book_df.groupby('time_id')['wap1'].apply(log_return)
    example_book_df.loc[:,('log_return2')] = example_book_df.groupby('time_id')['wap2'].apply(log_return)
    example_book_df = example_book_df[~example_book_df['log_return1'].isnull()] # drop the first null return value

    return example_book_df

example_book_df = order_book_data_feature_construction(path)
example_book_df

"""
1. **Feature Dictionary Definition**:
   ```python
   create_feature_dict = {
       'log_return1': [realized_volatility],
       'log_return2': [realized_volatility],
       'wap_balance': [np.mean],
       'price_spread': [np.mean],
       'bid_spread': [np.mean],
       'ask_spread': [np.mean],
       'volume_imbalance': [np.mean],
       'total_volume': [np.mean],
       'wap1': [np.mean],
       'wap2': [np.mean],
   }
   ```
   This dictionary defines the operations to be applied to each column of the DataFrame during the aggregation process. The keys represent the column names from `example_book_df`, and the values are lists of aggregation functions applied to these columns. For instance, `realized_volatility` is applied to both `log_return1` and `log_return2`, capturing the volatility of price movements. The `np.mean` function is used to compute the average for columns such as `wap_balance`, `price_spread`, and others, giving a typical value that summarizes the data across each time period.

2. **Grouping and Aggregation**:
   ```python
   df_feature = pd.DataFrame(example_book_df.groupby(['time_id']).agg(create_feature_dict)).reset_index()
   ```
   This line groups the data by `time_id` and then applies the specified aggregation functions. `time_id` typically represents different trading intervals or days, allowing you to capture and summarize the trading activity for each period separately.

3. **Renaming Columns**:
   ```python
   df_feature.columns = ['_'.join(col) for col in df_feature.columns]
   ```
   After aggregation, the resulting DataFrame columns become multi-level, where the top level is the original column names and the second level is the function names. This line flattens the columns by joining these levels with an underscore, making the column names more descriptive and easier to reference. For example, `log_return1_realized_volatility` and `wap1_mean`.

### Resulting DataFrame

The resulting `df_feature` DataFrame will have the following columns, with each column representing aggregated features calculated for each unique `time_id`:

- **time_id**: The identifier for the time interval.
- **log_return1_realized_volatility**: The realized volatility calculated from the first set of log returns.
- **log_return2_realized_volatility**: The realized volatility calculated from the second set of log returns.
- **wap_balance_mean**: The average weighted average price balance for the time interval.
- **price_spread_mean**: The average price spread during the time interval.
- **bid_spread_mean**: The average bid spread, which can indicate the tightness of the bid side of the market.
- **ask_spread_mean**: The average ask spread, reflecting conditions on the ask side.
- **volume_imbalance_mean**: The average volume imbalance, showing the difference between buying and selling pressures.
- **total_volume_mean**: The average total volume of traded stocks during the interval.
- **wap1_mean**: The average weighted average price at the first level.
- **wap2_mean**: The average weighted average price at the second level.
"""

create_feature_dict = {
        'log_return1':[realized_volatility],
        'log_return2':[realized_volatility],
        'wap_balance':[np.mean,],
        'price_spread':[np.mean],
        'bid_spread':[np.mean],
        'ask_spread':[np.mean],
        'volume_imbalance':[np.mean],
        'total_volume':[np.mean],
        'wap1':[np.mean],
        'wap2':[np.mean],
            }

df_feature = pd.DataFrame(example_book_df.groupby(['time_id']).agg(create_feature_dict)).reset_index()
df_feature.columns = ['_'.join(col) for col in df_feature.columns]

df_feature

"""- In usual financial market, we know trading volume is high in both opening hour and closing hour compare to other timing periods.

  Therefore, it might make sense to split the seconds in bucket and construct features

- First lets' see how many seconds in each time_id.

  From the below plot, it make sense to split the seconds in bucket to >=150, >=300, >=450.

Outlining a strategy to analyze the trading patterns throughout different times of the trading day, leveraging the granularity of the 'seconds in bucket' data to understand market dynamics better. The approach of splitting the data based on 'seconds in bucket' thresholds is a method to capture the variations in trading activity, particularly useful in the context of high-frequency trading data where market behaviors can significantly differ between the opening, midday, and closing phases.

### Understanding the Plot and Its Implications

The plot described uses a scatter plot to visualize each point in the `example_book_df` DataFrame, where:
- The x-axis (`time_id`) represents different time intervals or sessions within the trading day.
- The y-axis (`seconds in bucket`) shows the number of seconds elapsed since the start of trading at each `time_id`.

This visualization is helpful in spotting trends or patterns in trading activity over time. For instance, clusters of points at low and high values of `seconds in bucket` might indicate bursts of trading activity typically seen at market opening and closing times.

### Splitting the Data

Based on observation and the plotted data, the idea is to divide the `seconds in bucket` into segments that likely represent different market conditions:
- **Early Trading Period (0 to 150 seconds)**: Captures the opening minutes when trading volumes are typically high due to overnight news, early reactions, and other factors.
- **Mid-Session (150 to 300 seconds and 300 to 450 seconds)**: Might capture more of the steady-state trading conditions during the middle part of the trading window.
- **Late Trading Period (450+ seconds)**: Likely to encapsulate the closing rush, where traders adjust positions, and liquidity can surge again.

### Implementing the Feature Construction

To implement this in Python, you could extend your data preparation process to create aggregated features for these specific buckets. Here’s a concept for how to modify your DataFrame to reflect this segmentation:

```python
# Define buckets
buckets = [150, 300, 450]

# Group by 'time_id', then apply lambda to split 'seconds_in_bucket' according to defined buckets and aggregate other features as needed
grouped = example_book_df.groupby('time_id').apply(lambda x: pd.concat([
    x[x['seconds_in_bucket'] < buckets[0]].mean().add_suffix('_0_150'),
    x[(x['seconds_in_bucket'] >= buckets[0]) & (x['seconds_in_bucket'] < buckets[1])].mean().add_suffix('_150_300'),
    x[(x['seconds_in_bucket'] >= buckets[1]) & (x['seconds_in_bucket'] < buckets[2])].mean().add_suffix('_300_450'),
    x[x['seconds_in_bucket'] >= buckets[2]].mean().add_suffix('_450_plus')
])).reset_index()

# Flatten the columns after concatenation
grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]
```

This code snippet will create a new DataFrame where each `time_id` now has aggregated features for different segments of the trading session, providing a multi-faceted view of the market's behavior throughout the day. This approach can be incredibly beneficial for models that need to predict or analyze specific parts of the trading day differently, such as models focused on volatility, volume forecasting, or optimal trade execution timing.
"""

fig, ax = plt.subplots(figsize=(23,6))
ax.scatter(example_book_df['time_id'],example_book_df['seconds_in_bucket'],alpha=0.1)
ax.set_xlabel('time_id')
ax.set_ylabel('seconds in bucket')
plt.show()

"""
1. **Setting Time Intervals**:
   The code uses a loop to split the trading session into intervals of 150 seconds each, starting from 0 seconds. The intervals are used to segment the data into different periods of trading activity, which can be crucial for analyzing changes in market dynamics during the opening, middle, and closing phases.

   ```python
   seconds_in_bucket_interval = 150
   times_interval = 0
   for interval in range(4):
       print(times_interval)
       times_interval += seconds_in_bucket_interval
   ```

"""

seconds_in_bucket_interval = 150
times_interval = 0

for interval in range(4):

    print(times_interval)
    times_interval += seconds_in_bucket_interval

"""2. **Creating Features for Each Interval**:
   A DataFrame `df_book_feature` is initialized as empty. For each interval, the code filters the rows in `example_book_df` where `seconds_in_bucket` falls within the current interval. It then groups the data by `time_id` and applies aggregation functions defined in `create_feature_dict` to compute features like realized volatility, mean WAP balance, and others.

   ```python
   df_feature_sec = example_book_df[(example_book_df['seconds_in_bucket'] >= times_interval) & (example_book_df['seconds_in_bucket'] <= times_interval+150)]
   df_feature_sec = df_feature_sec.groupby(['time_id']).agg(create_feature_dict).reset_index()
   ```

3. **Renaming and Merging Features**:
   After computing the features for each interval, the columns are renamed to include the interval as a suffix, making them distinctive. These renamed feature sets are then merged into the `df_book_feature` DataFrame using `time_id` as the key. This ensures that features from different time intervals are aligned horizontally for each `time_id`.

   ```python
   df_feature_sec.columns = ['_'.join(col) + "_" + str(times_interval) for col in df_feature_sec.columns]
   df_feature_sec.columns = df_feature_sec.columns.str.replace('time_id__'+str(times_interval), 'time_id')

   if interval == 0:
       df_book_feature = df_feature_sec
   else:
       df_book_feature = pd.merge(df_book_feature, df_feature_sec, how='left', on='time_id')
   ```

### Resulting DataFrame and Its Implications

The final DataFrame `df_book_feature` contains a rich set of features for each `time_id`, with each set corresponding to a specific interval of seconds within the bucket. These features include:
- **Realized Volatility**: Measures the volatility for each interval, providing insight into how turbulent the market was during that period.
- **WAP Balance Mean**: Indicates the average balance of weighted average prices, useful for understanding market direction.
- **Price Spread Mean**: Offers an average of the price differences, which can suggest liquidity or market tightness.
- **Volume Imbalance Mean**: Shows the average difference between buying and selling volumes, which can indicate market pressure.

This structured approach to feature engineering allows for a nuanced analysis of market conditions at different times, making it possible to tailor trading strategies to specific phases of the trading session or to better predict market movements based on time-specific behaviors.

### Use Cases
- **Algorithmic Trading**: Traders can use these features to develop algorithms that adjust strategies based on the time-specific behaviors identified in the data.
- **Risk Management**: Understanding volatility and price movements within specific time intervals helps in managing the risk associated with high-frequency trading.
- **Market Analysis**: Analysts can study how different factors like volume imbalance and price spreads behave at different times to gauge market sentiment or predict future movements.

This method ensures that each time interval's unique characteristics are captured and analyzed, providing a deeper understanding of market dynamics throughout the trading day.
"""

df_book_feature = pd.DataFrame()
seconds_in_bucket_interval = 150
times_interval = 0

for interval in range(4):

    df_feature_sec = example_book_df[(example_book_df['seconds_in_bucket'] >= times_interval) & (example_book_df['seconds_in_bucket'] <= times_interval+150)]
    df_feature_sec = df_feature_sec.groupby(['time_id']).agg(create_feature_dict).reset_index()
    df_feature_sec.columns = ['_'.join(col) + "_" + str(times_interval) for col in df_feature_sec.columns]
    df_feature_sec.columns = df_feature_sec.columns.str.replace('time_id__'+str(times_interval), 'time_id')

    if interval == 0:
        df_book_feature = df_feature_sec
    else:
        df_book_feature = pd.merge(df_book_feature,df_feature_sec,how='left',left_on='time_id',right_on='time_id')

    times_interval += seconds_in_bucket_interval

print(df_book_feature.columns)
df_book_feature

"""Let's creates a heatmap to visualize the correlation between realized volatility of log returns at different time intervals (0, 150, 300, and 450 seconds) from the trading session. This kind of visualization is very useful in financial analysis to understand how volatilities at different parts of the trading session are interrelated. Here’s a breakdown and explanation of each step in code:

### Code Explanation

**Calculate Correlation**:
   ```python
   corr = df_book_feature[['log_return1_realized_volatility_0', 'log_return1_realized_volatility_150', 'log_return1_realized_volatility_300', 'log_return1_realized_volatility_450']].corr()
   ```
   This line computes the Pearson correlation coefficients between columns of realized volatilities at different time intervals. Correlation coefficients range from -1 to 1, where 1 means perfect positive correlation, -1 means perfect negative correlation, and 0 means no linear relationship.

**Plotting the Heatmap**:
   ```python
   plt.figure(figsize=(14, 6))
   heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='BrBG')
   ```
   - `plt.figure(figsize=(14, 6))` sets up a figure object with specified dimensions (14 inches wide by 6 inches tall).
   - `sns.heatmap()` creates the heatmap, where:
     - `corr` is the correlation matrix input.
     - `vmin` and `vmax` set the color scale's range for the heatmap.
     - `annot=True` enables annotations inside the squares showing the correlation values.
     - `cmap='BrBG'` specifies the color palette (Brown-Green) which is good for highlighting differences in the data.

The image displays a heatmap of correlation coefficients between realized volatilities of log returns calculated at different intervals within the trading session (0 seconds, 150 seconds, 300 seconds, and 450 seconds). From the plot, we can observe the following:

1. **Strong Positive Correlations**: All the correlation coefficients are positive and relatively high (ranging approximately from 0.73 to 0.85), indicating that there is a strong positive linear relationship between the realized volatilities at different time intervals. This suggests that if volatility is high during one interval, it is likely to be high in the others as well.

2. **Consistency Across Time Intervals**: The relatively uniform correlation coefficients across different intervals (no extremely low or negative values) imply that the market exhibits somewhat consistent behavior in terms of volatility throughout these intervals. This consistency can be important for trading strategies that assume stable volatility patterns.

3. **No Perfect Correlation**: While the correlations are strong, they are not perfect (not equal to 1), suggesting that there are differences in volatility patterns across different segments of the trading day. These differences can be exploited for timing trades or for risk management.

4. **Similarity in Adjacent Intervals**: The higher correlations between adjacent intervals (e.g., 0-150 seconds with 150-300 seconds) compared to non-adjacent intervals (e.g., 0-150 seconds with 300-450 seconds) suggest that volatility does not change abruptly in short succession but might evolve throughout the trading session.

5. **Potential for Volatility Clustering**: High correlations in financial time series often indicate volatility clustering, a phenomenon where high-volatility events are followed by high-volatility events, and low-volatility events are followed by low-volatility events. This could inform risk management strategies, as periods of high volatility could be expected to persist.

In conclusion, the heatmap analysis supports the notion that volatility exhibits time-dependent patterns which are relatively stable within the observed intervals but still exhibit some variation. Traders and quantitative analysts could leverage this information to understand and predict market behavior, potentially adjusting their algorithms and strategies to reflect these findings.




"""

import seaborn as sns

corr = df_book_feature[['log_return1_realized_volatility_0','log_return1_realized_volatility_150','log_return1_realized_volatility_300','log_return1_realized_volatility_450']].corr()

plt.figure(figsize=(14, 6))
# Store heatmap object in a variable to easily access it when you want to include more features (such as title).
# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.
heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True , cmap='BrBG' )
# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
heatmap.set_title('ETFs Monthly Return Correlation Matrix', fontdict={'fontsize':12}, pad=12) ;
plt.xticks(rotation=45)
plt.show()

"""The scatter plot visualizes the realized volatility of log returns at different seconds-in-bucket intervals (0 - 150, 150 - 300, 300 - 450, 450 - 600) against the `time_id`. Each `time_id` corresponds to a particular time interval or session, and the realized volatility is a measure of how much prices fluctuated during that session. Here are some conclusions we can draw from this plot:

1. **Distribution of Volatility**:
   - There is a dense clustering of data points towards the lower end of the volatility spectrum, indicating that high volatility is less common than low volatility within any given time interval.
   - The spread of volatility remains relatively consistent across different time intervals.

2. **Outliers**:
   - There are some notable outliers with significantly higher realized volatility. These could correspond to specific events or times when the market experienced higher-than-normal fluctuations.

3. **Comparison Across Time Intervals**:
   - No single interval consistently shows a significantly different volatility level compared to the others. The data points for different intervals are interspersed, suggesting that the distribution of volatility is somewhat uniform across different parts of the trading session.

4. **Time-Dependent Patterns**:
   - While the plot doesn't indicate any strong time-dependent patterns, there does appear to be a slight increase in the density of higher volatility data points in the later intervals (300 - 450, 450 - 600), but the difference is not stark. This might be expected if we consider traditional market behaviors where volatility can increase during the end of the trading session.

5. **Low Alpha (Transparency)**:
   - The use of `alpha=0.5` makes the points semi-transparent. Where we see a darker color, it indicates a greater number of overlapping points, which can help in identifying the areas with the most concentration of data points.

6. **Market Dynamics**:
   - The plot reinforces the idea that market volatility does not drastically differ across the specified time intervals on average, which could be an indication of a market that has a consistent behavior in volatility terms through the trading day, at least across the intervals observed here.

7. **Volume and Time Effects**:
   - The absence of a clear pattern might suggest that other factors, possibly unaccounted for in this plot, influence realized volatility. Traders might need to consider not just the time of day but also news events, market sentiment, and trading volume when assessing the risk and making decisions.

In conclusion, while the plot does not indicate strong differences in realized volatility based on these time buckets alone, it does provide a snapshot of market conditions at a granular time level. Traders and analysts might use this data in combination with other factors to further analyze market conditions and inform their strategies.
"""

fig,ax = plt.subplots(figsize=(23,6))
ax.set_title('Realized Volatility with Difference Seconds_in_Bucket')
ax.scatter(df_book_feature['time_id'],df_book_feature['log_return1_realized_volatility_0'],alpha=0.5,label='0~150')
ax.scatter(df_book_feature['time_id'],df_book_feature['log_return1_realized_volatility_150'],alpha=0.5,label='150~300')
ax.scatter(df_book_feature['time_id'],df_book_feature['log_return1_realized_volatility_300'],alpha=0.5,label='300~450')
ax.scatter(df_book_feature['time_id'],df_book_feature['log_return1_realized_volatility_450'],alpha=0.5,label='450~600')
ax.set_xlabel('time_id')
ax.set_ylabel('realized_volatility')
plt.legend()
plt.show()

"""# Trade book data EDA

DataFrame to contain trade-level data for a specific stock, identified as `stock_id=1`. The file is to have been read from a parquet file, which is a columnar storage file format often used for handling large-scale data.

Here’s a breakdown of the columns present in the DataFrame:

1. **time_id**: This is a unique identifier that likely corresponds to a specific time interval or trading session. It groups all trades that occurred within the same time frame.

2. **seconds_in_bucket**: This represents the number of seconds past the start of the session (denoted by `time_id`) at which the trades occurred. It's a finer time granularity within the `time_id`.

3. **trade_price**: This column records the price at which trades were executed. These are actual transaction prices as opposed to bid/ask prices listed in an order book.

4. **trade_size**: The number of shares, contracts, or financial instruments traded. This is the trade volume and can be an indication of the trade's impact on the market.

5. **trade_order_count**: The number of individual orders that were aggregated to the reported trade size. This can give a sense of market activity and liquidity. For example, a large trade size coming from many small orders might indicate a different market dynamic than the same size coming from a few large orders.

### EDA (Exploratory Data Analysis) on Trade Book Data

The initial EDA steps on such a dataset would likely include the following:

- **Descriptive Statistics**: Generate summary statistics to understand the distributions of `trade_price`, `trade_size`, and `trade_order_count`. This includes mean, median, standard deviation, min/max, and quantiles.

- **Time Series Analysis**: Examine the `trade_price` over `time_id` and `seconds_in_bucket` to visualize price movements within the trading session.

- **Volume Analysis**: Analyze `trade_size` to identify any spikes which could indicate significant market events or transactions.

- **Order Count Analysis**: Assess the `trade_order_count` to understand the composition of trade sizes and see if there are many small orders or a few large ones.

- **Correlation Analysis**: Investigate the relationship between `trade_price`, `trade_size`, and `trade_order_count` to see if larger trades affect prices more significantly.

- **Volatility Assessment**: Calculate the volatility of `trade_price` and compare it with `time_id` or specific `seconds_in_bucket` intervals to see how volatility changes over time.

- **Visualizations**: Use plots such as histograms, time series plots, scatter plots, and boxplots to visualize the distributions and trends in the data.

- **Anomaly Detection**: Look for outliers or anomalies in the data which might indicate errors, market manipulation, or significant events.

Conducting an EDA on this dataset can provide traders and analysts with valuable insights into market behavior, and can form the basis for developing trading strategies, algorithms, and risk management tools.
"""

example_trade_book_df = pd.read_parquet(r'trade_train.parquet/stock_id=1/170b39f1f7144bb3b4554aabc336106e.parquet')
example_trade_book_df.columns = ['time_id','seconds_in_bucket','trade_price','trade_size','trade_order_count']
example_trade_book_df

"""In this snippet, two new columns have been calculated and added to the `example_trade_book_df` DataFrame: `trade_per_size` and `trade_log_return`.

1. **trade_per_size**: This column represents the average trade size per order. It is calculated by dividing the `trade_size` by the `trade_order_count`. This metric can give insight into the average size of an individual order within each aggregated trade. A higher `trade_per_size` might suggest fewer, larger orders, whereas a lower `trade_per_size` might suggest a larger number of smaller orders.

2. **trade_log_return**: This column is the logarithmic return of the trade prices within each `time_id`. Logarithmic returns are commonly used in finance due to their desirable properties, such as time-additivity and handling of asymmetric effects of positive and negative returns. The log return between two prices is calculated as the natural logarithm of the second price divided by the first. In the DataFrame, the `log_return` function is applied to the `trade_price` for each group of `time_id`. This measure helps to analyze the percentage change in price from one trade to the next.

After calculating the `trade_log_return`, rows with null values for this new column are dropped. These nulls are a result of the `.diff()` operation within the `log_return` function, which does not return a value for the first row in each group since there is no previous price to calculate a difference from.

The updated DataFrame now contains additional information useful for understanding individual trade characteristics and price movements:

- **trade_per_size**: Averaged order size per trade could be relevant in assessing market impact, liquidity, and the participation of retail versus institutional investors.
- **trade_log_return**: Provides a quick assessment of the market's direction and momentum within the specific `time_id`. Consecutive positive log returns suggest a price uptrend, while negative log returns suggest a downtrend.

With these enhancements, the DataFrame is now a more powerful tool for exploratory data analysis, allowing for more nuanced insights into market dynamics and trade flows.
"""

example_trade_book_df.loc[:,('trade_per_size')] = example_trade_book_df['trade_size'] / example_trade_book_df['trade_order_count']
example_trade_book_df.loc[:,('trade_log_return')] = example_trade_book_df.groupby('time_id')['trade_price'].apply(log_return)
example_trade_book_df = example_trade_book_df[~example_trade_book_df['trade_log_return'].isnull()] # drop the first null return value
example_trade_book_df

"""This scatter plot visualizes the distribution of trades across different `time_id`s over the `seconds_in_bucket`. Here's what we can interpret from the image:

1. **Uniform Distribution of Trades**: The plot shows a dense and uniform red area, indicating that trades are evenly distributed throughout the time for each `time_id`. This could suggest that trading is constant and occurs regularly throughout the trading session.

2. **Trade Activity Throughout the Day**: Since there are data points spread across the entire range of `seconds_in_bucket`, it indicates that trades are happening at all times from the beginning to the end of each trading session.

3. **Gaps and Pauses in Trading**: The lighter vertical lines, where the red is less intense, may indicate moments with fewer trades, suggesting brief periods of inactivity or lower trading volume.

4. **Alpha Transparency**: The use of `alpha=0.1` makes individual points nearly transparent. Areas that appear darker are where many points overlap, suggesting a higher density of trades.

5. **No Clear Patterns Over Time**: The distribution appears random without any clear patterns or trends over different `time_id`s. This randomness implies that the trade frequency is not significantly changing over time across the different time intervals.

6. **High-Frequency Data**: The granularity and high frequency of data points are evident, typical of high-frequency trading datasets. This level of detail is necessary for microstructure analysis and algorithmic trading strategies.

7. **Data Sparsity**: Despite the overall uniformity, there seems to be some sparsity in trades at certain `time_id`s. These could either be a data artifact, natural market behavior, or could indicate times of day when trading activity is traditionally less dense, such as lunch hours or just before the close.

The key takeaway is that trading does not seem to concentrate at particular times within each session, and there are no immediately apparent anomalies or irregularities in trade timing. This plot provides a foundational understanding of trade distribution, which can be a starting point for more detailed time series analysis or to explore the impact of trades on price movements and volatility.
"""

fig, ax = plt.subplots(figsize=(23,6))
ax.set_title('trade book seconds_in_bucket per time_id')
ax.scatter(example_trade_book_df['time_id'],example_trade_book_df['seconds_in_bucket'],alpha=0.1)
ax.set_xlabel('time_id')
ax.set_ylabel('seconds in bucket')
plt.show()

"""The Python code snippet aggregates trade-level data to create higher-level features that summarize the activity within each trading interval, represented by `time_id`. It then creates a new DataFrame, `df_trade_feature`, with these summarized features. Let's explore the new features and their potential insights:

1. **trade_log_return_realized_volatility**: This feature is the realized volatility calculated from the logarithmic returns of trade prices within each `time_id`. It provides a measure of how much the price fluctuated during the trading interval and can be an indicator of market volatility during that period.

2. **trade_per_size_mean**: The average trade size per order for each `time_id`. This can indicate the typical market participation for each trade, with higher values potentially indicating larger institutional orders or block trades.

3. **trade_price_mean**: The mean trade price within each `time_id`. It gives a sense of the average level at which trades are executed during the interval and can be a reference point for price analysis over time.

4. **trade_order_count_mean**: The average number of individual orders that make up each trade within a `time_id`. A higher mean order count might suggest a more active or fragmented market with many small orders, while a lower mean could indicate fewer but larger trades.

5. **trade_size_mean**: The average trade size (volume) for each `time_id`. This figure can be used to gauge the intensity of trading activity and liquidity for that interval. Large average trade sizes may imply more significant market impact and liquidity consumption.

The `df_trade_feature` DataFrame provides a concise and informative summary of trading activity that can be very useful for various analyses, such as:

- **Time Series Analysis**: To observe how volatility, average trade size, and other factors change over different time intervals.
- **Market Impact Analysis**: To assess how trade sizes and order counts might impact the trade price within each interval.
- **Liquidity Analysis**: To evaluate the availability of liquidity and market depth during different trading sessions.
- **Modeling and Forecasting**: To create predictive models that could forecast future volatility or price levels based on past summarized trading behaviors.

The new DataFrame is streamlined and suitable for these analyses because it reduces the granularity of trade-level data into more manageable and meaningful statistical summaries.
"""

create_trade_book_feature_dict = {
        'trade_log_return':[realized_volatility],
        'trade_per_size':[np.mean],
        'trade_price':[np.mean],
        'trade_order_count':[np.mean],
        'trade_size':[np.mean],
            }

df_trade_feature = pd.DataFrame(example_trade_book_df.groupby(['time_id']).agg(create_trade_book_feature_dict)).reset_index()
df_trade_feature.columns = ['_'.join(col) for col in df_trade_feature.columns]

df_trade_feature

"""The scatter plot visualizes the relationship between the average size of individual trades (`trade_per_size_mean`) and the realized volatility of trade log returns (`trade_log_return_realized_volatility`). Here are some observations and possible conclusions from the plot:

1. **Concentration of Data Points**: Most data points are concentrated in the lower left part of the plot, indicating that smaller trade sizes are associated with a wide range of volatilities, but generally, smaller trade sizes do not lead to extremely high volatility.

2. **Outliers**: There are a few outliers with higher trade sizes and realized volatilities. This could suggest that occasionally, larger trades might be associated with higher price volatility, potentially due to larger trades having a more significant impact on the market.

3. **Trend Absence**: There does not appear to be a strong or clear trend or correlation between trade size and realized volatility across the majority of data points. If larger trade sizes systematically led to higher volatility, we would expect to see a gradient or pattern, which is not evident here.

4. **Sparse High Volatility**: Higher realized volatility levels (above 0.01) are relatively sparse and do not appear to be concentrated around any particular trade size. This suggests that high volatility is not common and is not tightly linked to the average size of trades.

5. **Distribution of Trade Sizes**: Most of the trade sizes are within a smaller range (below 50 in this case), with very few trades reaching larger sizes. This is indicative of the market structure or the typical behavior of traders in this market.

6. **Potential Market Impact**: While the plot does not show a definitive relationship, the presence of outliers might warrant a more detailed investigation into whether there are specific conditions or contexts where larger trades do impact volatility more significantly.

In conclusion, the plot suggests that, for this dataset, larger trade sizes do not consistently lead to higher realized volatility. However, the presence of outliers with both large trade sizes and high volatility indicates that under certain conditions, larger trades may indeed influence price movements more dramatically. This kind of analysis is essential for risk management and for designing algorithms that minimize market impact.
"""

fig,ax = plt.subplots(figsize=(23,6))
ax.scatter(df_trade_feature['trade_per_size_mean'],df_trade_feature['trade_log_return_realized_volatility'])
ax.set_xlabel('trade per size')
ax.set_ylabel('trade log realized volatility')
plt.show()

"""1. **Initialization**: The `df_trade_feature` DataFrame is initialized as an empty DataFrame that will be used to store the aggregated features for each interval.

2. **Setting Time Intervals**: The loop iterates four times, each representing a different 150-second interval in the `seconds_in_bucket`.

3. **Aggregation per Interval**: Within each iteration, the code performs the following actions:
   - Filters the trades that occurred at or after the current `times_interval`.
   - Groups the filtered data by `time_id`.
   - Aggregates the group using the dictionary of functions provided (`create_trade_book_feature_dict`), which calculates features like realized volatility and average trade size.
   - Resets the index to turn the group identifiers (`time_id`) back into a column.

4. **Renaming Columns**: It renames the columns to reflect the interval they were calculated for, appending the `times_interval` value to the column names.

5. **Merging DataFrames**: It merges the interval-specific features back into the main `df_trade_feature` DataFrame. The merge is done on `time_id`, and it uses a left join to ensure all `time_id`s are kept in the resulting DataFrame.

6. **Increment Time Interval**: The `times_interval` is incremented by 150 to move to the next interval.

7. **Printing Column Names**: After the loop, the column names of the resulting DataFrame are printed.

8. **Resulting DataFrame**: The final DataFrame `df_trade_feature` includes the features for each interval, with each set of features indexed by `time_id`.

### Resulting DataFrame Structure

The resulting DataFrame, `df_trade_feature`, has columns corresponding to each interval's aggregated statistics, such as:
- Realized volatility of log returns
- Mean trade size per order
- Mean trade price
- Mean trade order count
- Mean trade size

The columns are suffixed with the time interval they correspond to (e.g., `_0`, `_150`, `_300`, `_450`).

### Insights and Usage

This DataFrame can provide insights into how trading behavior changes over different intervals within the trading session. For example, analysts might look for patterns like whether volatility tends to increase or decrease as the session progresses, or if the average trade size varies significantly at different times. Such information could be used for developing trading strategies, adjusting risk management practices, or gaining a deeper understanding of market dynamics.

By having these features broken down by time intervals, it's possible to conduct a time-of-day analysis, which could reveal intraday patterns in trade activity and market movements.
"""

df_trade_feature = pd.DataFrame()
seconds_in_bucket_interval = 150
times_interval = 0

for interval in range(4):

    df_trade_feature_sec = pd.DataFrame(example_trade_book_df.query(f'seconds_in_bucket >= {times_interval}').groupby(['time_id']).agg(create_trade_book_feature_dict)).reset_index()
    df_trade_feature_sec.columns = ['_'.join(col) + "_" + str(times_interval) for col in df_trade_feature_sec.columns]
    df_trade_feature_sec.columns = df_trade_feature_sec.columns.str.replace('time_id__'+str(times_interval), 'time_id')

    if interval == 0:
        df_trade_feature = df_trade_feature_sec
    else:
        df_trade_feature = pd.merge(df_trade_feature,df_trade_feature_sec,how='left',left_on='time_id',right_on='time_id')

    times_interval += seconds_in_bucket_interval

print(df_trade_feature.columns)
df_trade_feature

"""# Join Features DataFrame

The `join_book_df` DataFrame is created by merging two separate DataFrames: `df_book_feature`, which contains features based on the order book data, and `df_trade_feature`, which contains features based on trade data. The merge is conducted on the common key `time_id`, and `fillna(value=0)` is used to replace any NaN values resulting from the join operation with 0. This is a typical step in data preparation, particularly when combining datasets that may not have a perfect one-to-one match on the key being used for the join.

The resulting DataFrame now contains a comprehensive set of features that describe both the order book and trade aspects of the market for each `time_id`. It includes variables such as realized volatility from log returns, various mean calculations (like weighted average price balance, price spread, bid spread, ask spread), trade size, and order count, each for different time buckets within the trading day.

### Insights and Usage:

1. **Complete Market Picture**: The merged DataFrame offers a more holistic view of market behavior, combining information about both pending orders (from the order book) and completed trades.

2. **Data Imputation**: Filling missing values with zero is a practical choice, assuming that if data is missing, it could imply no activity or a neutral value for the feature. However, this choice should be considered carefully, as it could introduce bias or inaccuracies, especially if missing data has a different underlying cause.

3. **Feature Engineering**: This DataFrame is ripe for further feature engineering, such as creating ratios or differences between order book and trade features, which could provide additional insights into the interaction between market liquidity (order book) and market activity (trades).

4. **Predictive Modeling**: The combined features can be used for predictive models that aim to forecast market movements, such as the direction of the next price move or the volatility in the next time interval.

5. **Risk Analysis**: The features related to volatility can be particularly useful for risk management purposes, allowing for the estimation of value at risk (VaR) or expected shortfall (ES) metrics that consider both pending orders and completed trades.

6. **Strategy Development**: Trading strategies can be developed or refined using these features, such as strategies that might leverage the relationship between order size and price impact or the effects of market depth on price volatility.
"""

join_book_df = pd.merge(df_book_feature,df_trade_feature,left_on=['time_id'],right_on=['time_id'],how='left').fillna(value=0)
join_book_df

"""# Function and pipeline all the process

The function `book_data_preprocess_per_stock` is a comprehensive pipeline that processes the order book data for a given stock identified by its `stock_id` in the file path. The pipeline performs feature extraction and aggregation to create a structured DataFrame suitable for further analysis or model input.

Here's an outline of the pipeline process:

1. **Feature Construction**:
   - Compute bid-ask spreads, total volume, volume imbalance, and weighted average prices (WAP1 and WAP2) from the raw order book data.
   - Calculate log returns from WAPs to reflect price movements.

2. **Feature Aggregation**:
   - Group data by `time_id` and aggregate using predefined functions such as realized volatility and mean calculations for several features.

3. **Segmentation into Time Intervals**:
   - Divide the trading session into 150-second intervals and repeat the feature aggregation process for each segment, ensuring that the trading day's microstructure is captured.

4. **Merging Aggregated Features**:
   - Merge features from all intervals into a single DataFrame where each `time_id` is represented by its comprehensive set of features.

5. **Creating a Unified DataFrame**:
   - Add a `row_id` that combines the `stock_id` with the `time_id` to uniquely identify each row.
   - Drop the `time_id` column if it's no longer needed after adding `row_id`.

6. **Handling Missing Values**:
   - Fill any resulting NaN values (from the merge operations) with zeros, which assumes that missing data indicates no activity or neutral value for a feature.

7. **Output**:
   - The final output is `df_book_feature`, a DataFrame ready for analysis, visualization, or as input to machine learning models.

The function `book_data_preprocess_per_stock` can be called with the file path to the order book data for a specific stock to perform all the above steps and return the processed features.

For practical usage, you could loop through multiple stock files and apply this function to each one to construct a comprehensive dataset with features across multiple stocks. This dataset could then be used for cross-sectional analysis across different securities or to feed into a predictive model that aims to forecast market movements based on order book dynamics.
"""

path = r'./book_train.parquet/stock_id=0'


def log_return(list_stock_prices):
    return np.log(list_stock_prices).diff()

def realized_volatility(series_log_return):
    return np.sqrt(np.sum(series_log_return**2))

def order_book_data_feature_construction(path):

    stock_id = path.split("=")[1]
    example_book_df = pd.read_parquet(path)

    # construct item for realized volatility calculation
    example_book_df.loc[:,('ask_spread')] = example_book_df['ask_size1'] - example_book_df['ask_size2']
    example_book_df.loc[:,('bid_spread')] = example_book_df['bid_size1'] - example_book_df['bid_size2']
    example_book_df.loc[:,('total_volume')] = example_book_df['ask_size1'] + example_book_df['ask_size2'] + example_book_df['bid_size1'] + example_book_df['bid_size2']
    example_book_df.loc[:,('volume_imbalance')] = abs(example_book_df['ask_size1'] + example_book_df['ask_size2'] - example_book_df['bid_size1'] - example_book_df['bid_size2'] )


    example_book_df.loc[:,('price_spread')] = (example_book_df['ask_price1']/example_book_df['bid_price1'])-1 # Constrcut Bid/Ask Spread.
    example_book_df.loc[:,('wap1')] = (example_book_df['bid_price1']*example_book_df['ask_size1'] + example_book_df['ask_price1']*example_book_df['bid_size1']) / (example_book_df['ask_size1'] + example_book_df['bid_size1'])
    example_book_df.loc[:,('wap2')] = (example_book_df['bid_price2']*example_book_df['ask_size2'] + example_book_df['ask_price2']*example_book_df['bid_size2']) / (example_book_df['ask_size2'] + example_book_df['bid_size2'])
    example_book_df.loc[:,('wap_balance')] = example_book_df['wap1'] - example_book_df['wap2']

    example_book_df.loc[:,('log_return1')] = example_book_df.groupby('time_id')['wap1'].apply(log_return)
    example_book_df.loc[:,('log_return2')] = example_book_df.groupby('time_id')['wap2'].apply(log_return)
    example_book_df = example_book_df[~example_book_df['log_return1'].isnull()] # drop the first null return value

    return example_book_df


def book_data_preprocess_per_stock(path):

    stock_id = path.split("=")[1]
    example_book_df = order_book_data_feature_construction(path)

    create_feature_dict = {
            'log_return1':[realized_volatility],
            'log_return2':[realized_volatility],
            'wap_balance':[np.mean,],
            'price_spread':[np.mean],
            'bid_spread':[np.mean],
            'ask_spread':[np.mean],
            'volume_imbalance':[np.mean],
            'total_volume':[np.mean],
            'wap1':[np.mean],
            'wap2':[np.mean],
                }

    df_feature = pd.DataFrame(example_book_df.groupby(['time_id']).agg(create_feature_dict)).reset_index()
    df_feature.columns = ['_'.join(col) for col in df_feature.columns]

    df_book_feature = pd.DataFrame()
    seconds_in_bucket_interval = 150
    times_interval = 0

    for interval in range(4):

        df_feature_sec = pd.DataFrame(example_book_df.query(f'seconds_in_bucket >= {times_interval}').groupby(['time_id']).agg(create_feature_dict)).reset_index()
        df_feature_sec.columns = ['_'.join(col) + "_" + str(times_interval) for col in df_feature_sec.columns]
        df_feature_sec.columns = df_feature_sec.columns.str.replace('time_id__'+str(times_interval), 'time_id')

        if interval == 0:
            df_book_feature = df_feature_sec
        else:
            df_book_feature = pd.merge(df_book_feature,df_feature_sec,how='left',left_on='time_id',right_on='time_id')

        times_interval += seconds_in_bucket_interval

    df_book_feature.insert(0,'row_id' ,[f'{stock_id}-{x}' for x in df_book_feature['time_id'].to_list()])
    df_book_feature = df_book_feature.drop(['time_id'],axis=1)

    return df_book_feature


book_data_preprocess_per_stock(path)

"""The provided pipeline for trade data includes functions for reading, processing, and aggregating trade book data. The pipeline involves reading the trade data, constructing features, segmenting the data into 150-second intervals, and then merging these features to create a final DataFrame that summarizes the trade data across these intervals.

Here's a detailed overview of the steps within the `trade_data_preprocess_per_stock` function:

1. **Reading Trade Data**:
   - Load the parquet file for the given stock_id using `pd.read_parquet`.
   - Rename the columns for clarity and consistency.

2. **Feature Construction**:
   - Calculate the `trade_per_size` by dividing the trade size by the number of orders.
   - Compute the `trade_log_return` by applying the `log_return` function to the `trade_price` grouped by `time_id`.

3. **Feature Aggregation**:
   - Aggregate various statistics (realized volatility, mean of trade per size, trade price, trade order count, and trade size) at the `time_id` level.

4. **Segmentation and Aggregation by Time Intervals**:
   - Create aggregated features for each of the 150-second intervals within the trading session.
   - Merge these interval-specific features into a single DataFrame.

5. **Merging Aggregated Features Across Intervals**:
   - Use a loop to perform the aggregation for each interval.
   - Merge the resulting interval-specific DataFrames on `time_id`.

6. **Formatting the Final DataFrame**:
   - Insert a `row_id` column at the beginning of the DataFrame that uniquely identifies each row based on `stock_id` and `time_id`.
   - Drop the original `time_id` column as it is no longer needed after adding `row_id`.

7. **Returning the Final DataFrame**:
   - The final DataFrame `df_trade_feature` includes interval-specific aggregated features and is ready for analysis or modeling.

To use this pipeline, you would call the `trade_data_preprocess_per_stock` function with the file path to the trade data for a specific stock. The function would then process the data and return a DataFrame containing the processed and aggregated features.

This pipeline is highly useful for quantitative analysis in finance, as it encapsulates the entire process of feature extraction and preparation from raw, high-frequency trade data, allowing for a structured approach to modeling and analysis. The resulting DataFrame could be used to identify patterns, develop trading strategies, or feed into predictive models to forecast market movements.
"""

trade_path = r'trade_train.parquet/stock_id=0'

def trade_book_data_feature_construction(path):

    example_trade_book_df = pd.read_parquet(path)
    example_trade_book_df.columns = ['time_id','seconds_in_bucket','trade_price','trade_size','trade_order_count']


    example_trade_book_df.loc[:,('trade_per_size')] = example_trade_book_df['trade_size'] / example_trade_book_df['trade_order_count']
    example_trade_book_df.loc[:,('trade_log_return')] = example_trade_book_df.groupby('time_id')['trade_price'].apply(log_return)
    example_trade_book_df = example_trade_book_df[~example_trade_book_df['trade_log_return'].isnull()] # drop the first null return value

    return example_trade_book_df


def trade_data_preprocess_per_stock(path):

    stock_id = path.split("=")[1]
    example_trade_book_df = trade_book_data_feature_construction(path)

    create_trade_book_feature_dict = {
            'trade_log_return':[realized_volatility],
            'trade_per_size':[np.mean],
            'trade_price':[np.mean],
            'trade_order_count':[np.mean],
            'trade_size':[np.mean],
                }

    df_trade_feature = pd.DataFrame(example_trade_book_df.groupby(['time_id']).agg(create_trade_book_feature_dict)).reset_index()
    df_trade_feature.columns = ['_'.join(col) for col in df_trade_feature.columns]

    df_trade_feature = pd.DataFrame()
    seconds_in_bucket_interval = 150
    times_interval = 0

    for interval in range(4):

        df_trade_feature_sec = pd.DataFrame(example_trade_book_df.query(f'seconds_in_bucket >= {times_interval}').groupby(['time_id']).agg(create_trade_book_feature_dict)).reset_index()
        df_trade_feature_sec.columns = ['_'.join(col) + "_" + str(times_interval) for col in df_trade_feature_sec.columns]
        df_trade_feature_sec.columns = df_trade_feature_sec.columns.str.replace('time_id__'+str(times_interval), 'time_id')

        if interval == 0:
            df_trade_feature = df_trade_feature_sec
        else:
            df_trade_feature = pd.merge(df_trade_feature,df_trade_feature_sec,how='left',left_on='time_id',right_on='time_id')

        times_interval += seconds_in_bucket_interval

    df_trade_feature.insert(0,'row_id' ,[f'{stock_id}-{x}' for x in df_trade_feature['time_id'].to_list()])
    df_trade_feature = df_trade_feature.drop(['time_id'],axis=1)

    return df_trade_feature

trade_data_preprocess_per_stock(trade_path)

"""The `main_data_preprocess` function consolidates the entire preprocessing pipeline for both the book and trade data for a given stock. It serves as the main entry point to execute the feature engineering steps defined in the previous functions and merge the resulting features into a single DataFrame. Here's a breakdown of how this function operates:

1. **Process Book Data**: Calls `book_data_preprocess_per_stock` with the `book_path` to read and preprocess the order book data. This generates `df_book_features` containing engineered features from the order book.

2. **Process Trade Data**: Similarly, calls `trade_data_preprocess_per_stock` with the `trade_path` to preprocess the trade data. This creates `df_trade_features` with features derived from the trade data.

3. **Merge Features**: Merges the two feature sets (`df_book_features` and `df_trade_features`) on the common `row_id` column using a left join. This aligns the book and trade data features for each `time_id` (now embedded within `row_id`).

4. **Return Unified DataFrame**: The function returns the merged DataFrame, which now includes a wide array of features suitable for in-depth analysis or modeling.

### Output

The resulting `feature_per_stock` DataFrame contains:

- A unique `row_id` for each record, combining `stock_id` and `time_id`.
- Features from both the order book data and the trade data, each corresponding to different time intervals. This could include features like realized volatility, mean prices, mean trade sizes, and more.
"""

def main_data_preprocess(book_path,trade_path):

    df_book_features = book_data_preprocess_per_stock(book_path)
    df_trade_features = trade_data_preprocess_per_stock(trade_path)

    return pd.merge(df_book_features,df_trade_features,how='left',left_on='row_id',right_on='row_id')


book_path = r'./book_train.parquet/stock_id=0'
trade_path = r'trade_train.parquet/stock_id=0'


feature_per_stock = main_data_preprocess(book_path,trade_path)
feature_per_stock

"""# Joblib parallel conputing every stock features.

- list of stock id

To implement parallel computing for processing each stock's features using the `joblib` library, you'll first compile a list of stock IDs as you have started doing. Then you can define a function that will apply your `main_data_preprocess` function to each stock ID's data files in parallel. Here's an example of how you might set this up:

Firstly, make sure you have `joblib` installed. If not, you can install it using pip:

```sh
pip install joblib
```

Then, you can use the following code to process each stock in parallel:

```python
from joblib import Parallel, delayed
import glob

# Function to apply to each stock
def process_stock_data(stock_id):
    book_path = f'./book_train.parquet/stock_id={stock_id}'
    trade_path = f'trade_train.parquet/stock_id={stock_id}'
    return main_data_preprocess(book_path, trade_path)

# Get the list of stock ids from the order book files
list_order_book_file_train = glob.glob(r'./book_train.parquet/*')
list_stock_ids = [path.split("=")[1] for path in list_order_book_file_train]

# Perform parallel processing
# n_jobs is the number of CPU cores to use; -1 means using all available cores
features_per_stock_list = Parallel(n_jobs=-1)(delayed(process_stock_data)(stock_id) for stock_id in list_stock_ids)

# Concatenate all the DataFrames into one DataFrame
features_per_stock = pd.concat(features_per_stock_list, ignore_index=True)

print(features_per_stock)
```

Here’s what this script does:

1. **Defines a Function for Processing**: `process_stock_data` is a function that takes a `stock_id`, constructs the paths for book and trade data, and then calls `main_data_preprocess`.

2. **Finds All Stock IDs**: Uses `glob.glob` to create a list of file paths for order book data, and then extracts the `stock_id` from each path.

3. **Parallel Processing**: Calls `Parallel` with `delayed` to process each stock's data in parallel. The number of jobs (`n_jobs=-1`) is set to use all available CPU cores for maximum efficiency.

4. **Combines Results**: Concatenates all the individual DataFrames returned by `process_stock_data` into a single DataFrame called `features_per_stock`.

5. **Prints the Result**: Outputs the combined DataFrame.

This approach is highly efficient for processing large datasets, especially when dealing with high-frequency financial data for multiple stocks. It leverages all available computational resources to speed up the processing time.
"""

import glob

list_order_book_file_train = glob.glob(r'./book_train.parquet/*')
list_stock_ids = [ path.split("=")[1] for path in list_order_book_file_train]

print(list_stock_ids)

"""The `preprocessor` function provided in your code is designed to process stock data in parallel using `joblib`. It processes both book and trade data for each stock ID, combining them into a single DataFrame. Here is a breakdown of how the function operates:

1. **Parallel Processing**: The `for_joblib` function inside `preprocessor` is called for each stock ID in the `list_stock_ids`. This function is the workhorse that reads the book and trade data for each stock, processes them using the `main_data_preprocess` function, and returns the features.

2. **Handling Warnings**: The warning from `loky.process_executor` suggests that some of the workers may have stopped unexpectedly. This could be due to several reasons, such as running out of memory or a timeout. It might be worth checking the resources of your system to ensure that each worker has enough memory and that the timeout settings are appropriate for the task.

3. **Merging Results**: After all parallel tasks are finished, the resulting DataFrames are concatenated into one large DataFrame, `df`, and sorted by `row_id`.

4. **Output**: The function returns a DataFrame with rows sorted by `row_id`, which should ensure that the data is in a predictable order for subsequent analysis or modeling.

Here are some considerations and tips for using this function effectively:

- Ensure sufficient memory is available for all the parallel processes.
- Check if the worker timeout needs to be adjusted based on the complexity of the data processing.
- Consider the number of cores and amount of RAM available on the machine to avoid overloading the system, which could cause workers to crash.
- Ensure that your data is clean and properly formatted before processing. Bad data could cause workers to fail.
- Verify that the `data_dir` variable is set to the correct directory where your data is located.
- Use robust error handling within the `for_joblib` function to catch and log any issues with individual stocks, which can help diagnose issues that cause workers to stop.

If the processing requires heavy computation or a lot of memory, consider using a machine with more resources, or optimize your data processing functions to be more efficient.
"""

from joblib import Parallel, delayed

# File Case for data path file case !
data_dir = r'./'

# Funtion to make preprocessing function in parallel (for each stock id)
def preprocessor(list_stock_ids):

    def for_joblib(stock_id):

        book_path  = data_dir + "book_train.parquet/stock_id="  + str(stock_id)
        trade_path = data_dir + "trade_train.parquet/stock_id=" + str(stock_id)

        feature_per_stock = main_data_preprocess(book_path,trade_path)

        return feature_per_stock

    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)
    df = pd.concat(df,ignore_index=True)

    return df.sort_values(by='row_id')

train_features_df = preprocessor(list_stock_ids)
train_features_df

"""The `To_Pickle` function is designed to serialize a Python object into a file using the `pickle` module. This allows for easy storage of complex data structures, such as DataFrames, that can later be deserialized back into Python objects with their original structure intact. Similarly, `Read_Pickle` is used to deserialize the pickle file back into a Python object.

Here's a summary of how each function works:

- **To_Pickle**:
  - Takes a Python dictionary and a file name as inputs.
  - Opens a new file with the specified file name and `.pickle` extension in write-binary mode.
  - Dumps the dictionary into the pickle file.
  - Closes the file to ensure data is written properly.

- **Read_Pickle**:
  - Takes a file path as an input.
  - Opens the specified pickle file in read-binary mode.
  - Loads the pickle file content back into a Python object.
  - Returns the Python object, which could be any serialized object, such as a DataFrame.

In the given example, `train_features_df` (presumably a DataFrame) is serialized into a pickle file named `"all_feature_df.pickle"`. It's then read back into Python and presumably stored back in a variable, which would give you the original DataFrame structure.

Here are a few things to note about using `pickle`:

- Pickle files can be platform-dependent and may not be secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.
- Pickle uses a binary format. Care should be taken with file modes when writing and reading to ensure the data is handled correctly.
- Pickle is Python-specific. If you need to share data with programs written in other languages, consider using a format like JSON or CSV for text data, or HDF5 for larger numerical datasets.

Using `pickle` is convenient for quick serialization of Python objects, but in the context of data science, formats like CSV or Parquet are often preferred for data storage due to their interoperability and efficiency.
"""

import pickle

def To_Pickle(dict,file_name):

    pickle_out = open(str(file_name)+".pickle","wb")
    pickle.dump(dict,pickle_out)

    pickle_out.close()

def Read_Pickle(file_path):

    pickle_in = open(file_path,"rb")
    pickle_in  = pickle.load(pickle_in)

    return pickle_in


To_Pickle(dict=train_features_df.reset_index(drop=True) ,file_name="all_feature_df")
Read_Pickle(r"all_feature_df.pickle")